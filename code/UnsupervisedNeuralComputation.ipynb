{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised neural computation - Practical\n",
    "\n",
    "Dependencies:\n",
    "- Python (>= 2.6 or >= 3.3)\n",
    "- NumPy (>= 1.6.1)\n",
    "- SciPy (>= 0.12)\n",
    "- SciKit Learn (>=0.18.1)\n",
    "\n",
    "Just as there are different ways in which we ourselves learn from our own surrounding environments, so it is with neural networks. In a broad sense, we may categorize the learning processes through which neural networks function as follows: learning with a teacher and learning without a teacher. \n",
    "\n",
    "These different forms of learning as performed on neural networks parallel those of human learning. Learning with a teacher is also referred to as supervised learning. In conceptual terms, we may think of the teacher as having knowledge of the environment, with that knowledge being represented by a set of input - output examples. Unsupervised learning does not require target vectors for the outputs. \n",
    "\n",
    "Without input-output training pairs as external teachers, unsupervised learning is self-organized to produce consistent output vectors by modifying weights. That is to say, there are no labelled examples of the function to be learned by the network. \n",
    "\n",
    "For a specific task-independent measure, once the network has become tuned to the statistical regularities of the input data, the network develops the ability to discover internal structure for encoding features of the input or compress the input data, and thereby to create new classes automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Functions and Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function (typically Gaussian) used in various kernelized learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing the basic RBF parametrization\n",
    "# based on code from https://github.com/jeffheaton/aifh\n",
    "import numpy as np\n",
    "\n",
    "class RbfFunction(object):\n",
    "    def __init__(self, dimensions, params, index):\n",
    "        self.dimensions = dimensions\n",
    "        self.params = params\n",
    "        self.index = index\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.params[self.index]\n",
    "\n",
    "    @width.setter\n",
    "    def width(self, value):\n",
    "        self.params[self.index] = value\n",
    "\n",
    "    def set_center(self, index, value):\n",
    "        self.params[self.index + index + 1] = value\n",
    "\n",
    "    def get_center(self, index):\n",
    "        return self.params[self.index + index + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBFs can take various shapes: quadratic, multi-quadratic, inverse multi-quadratic, mexican hat. Yet the most used is the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing a Gaussian RBF\n",
    "class RbfGaussian(RbfFunction):\n",
    "    def evaluate(self, x):\n",
    "        value = 0\n",
    "        width = self.width\n",
    "\n",
    "        for i in range(self.dimensions):\n",
    "            center = self.get_center(i)\n",
    "            value += ((x[i] - center) ** 2) / (2.0 * width * width)\n",
    "        return np.exp(-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RBF network is an advanced machine learning algorithm that uses a series of RBF functions to perform regression.  It can also perform classification by means of one-of-n encoding. The long term memory of a RBF network is made up of the widths and centers of the RBF functions, as well as input and output weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing a Gaussian RBF Network\n",
    "class RbfNetwork(object):\n",
    "\n",
    "    def __init__(self, input_count, rbf_count, output_count):\n",
    "        \"\"\" Create an RBF network with the specified shape.\n",
    "        @param input_count: The input count.\n",
    "        @param rbf_count: The RBF function count.\n",
    "        @param output_count:  The output count.\n",
    "        \"\"\"\n",
    "        self.input_count = input_count\n",
    "        self.output_count = output_count\n",
    "\n",
    "        # calculate input and output weight counts\n",
    "        # add 1 to output to account for an extra bias node\n",
    "        input_weight_count = input_count * rbf_count\n",
    "        output_weight_count = (rbf_count + 1) * output_count\n",
    "        rbf_params = (input_count + 1) * rbf_count\n",
    "        self.long_term_memory = np.zeros((input_weight_count + output_weight_count + rbf_params), dtype=float)\n",
    "\n",
    "        self.index_input_weights = 0\n",
    "        self.index_output_weights = input_weight_count + rbf_params\n",
    "\n",
    "        self.rbf = {}\n",
    "\n",
    "        # default the Rbf's to gaussian\n",
    "        for i in range(0, rbf_count):\n",
    "            rbf_index = input_weight_count + ((input_count + 1) * i)\n",
    "            self.rbf[i] = RbfGaussian(input_count, self.long_term_memory, rbf_index)\n",
    "\n",
    "\n",
    "    def compute_regression(self, input):\n",
    "        \"\"\" Compute the output for the network.\n",
    "        @param input: The input pattern.\n",
    "        @return: The output pattern.\n",
    "        \"\"\"\n",
    "        # first, compute the output values of each of the RBFs\n",
    "        # Add in one additional RBF output for bias (always set to one).\n",
    "        rbf_output = [0] * (len(self.rbf) + 1)\n",
    "        # bias\n",
    "        rbf_output[len(rbf_output) - 1] = 1.0\n",
    "\n",
    "        for rbfIndex in range(0, len(self.rbf)):\n",
    "            # weight the input\n",
    "            weighted_input = [0] * len(input)\n",
    "\n",
    "            for inputIndex in range(0, len(input)):\n",
    "                memory_index = self.index_input_weights + (rbfIndex * self.input_count) + inputIndex\n",
    "                weighted_input[inputIndex] = input[inputIndex] * self.long_term_memory[memory_index]\n",
    "\n",
    "            # calculate the rbf\n",
    "            rbf_output[rbfIndex] = self.rbf[rbfIndex].evaluate(weighted_input)\n",
    "\n",
    "        # Second, calculate the output, which is the result of the weighted result of the RBF's.\n",
    "        result = [0] * self.output_count\n",
    "\n",
    "        for outputIndex in range(0, len(result)):\n",
    "            sum_value = 0\n",
    "            for rbfIndex in range(0, len(rbf_output)):\n",
    "                # add 1 to rbf length for bias\n",
    "                memory_index = self.index_output_weights + (outputIndex * (len(self.rbf) + 1)) + rbfIndex\n",
    "                sum_value += rbf_output[rbfIndex] * self.long_term_memory[memory_index]\n",
    "            result[outputIndex] = sum_value\n",
    "\n",
    "        # finally, return the result.\n",
    "        return result\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the network to a random state.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(self.long_term_memory)):\n",
    "            self.long_term_memory[i] = np.random.uniform(0, 1)\n",
    "\n",
    "    def compute_classification(self, input):\n",
    "        \"\"\" Compute the output and return the index of the output with the largest value.  This is the class that\n",
    "        the network recognized.\n",
    "        @param input: The input pattern.\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        output = self.compute_regression(input)\n",
    "        return output.index(max(output))\n",
    "\n",
    "    def copy_memory(self, source):\n",
    "        \"\"\" Copy the specified vector into the long term memory of the network.\n",
    "        @param source: The source vector.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(source)):\n",
    "            self.long_term_memory[i] = source[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a traditional benchmark in classification problems in ML. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems\" as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Based on Fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques in machine learning such as support vector machines.\n",
    "\n",
    "In the following we will use simulated annealing to fit an RBF network to the Iris data set, to classifiy the iris species correctly.\n",
    "\n",
    "Simulated annealing is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file: /media/data_warehouse/Dropbox/Public/sandbox/teaching/Basecamp.ai_Lectures/code/data/iris.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the dataset\n",
    "import os\n",
    "import sys\n",
    "from normalize import Normalize\n",
    "from error import ErrorCalculation\n",
    "from train import TrainAnneal\n",
    "import numpy as np\n",
    "\n",
    "irisFile = os.path.abspath(\"./data/iris.csv\")\n",
    "\n",
    "# Read the Iris data set\n",
    "print('Reading CSV file: ' + irisFile)\n",
    "norm = Normalize()\n",
    "iris_work = norm.load_csv(irisFile)\n",
    "\n",
    "# Extract the original iris species so we can display during the final validation\n",
    "ideal_species = [row[4] for row in iris_work]\n",
    "\n",
    "# Setup the first four fields to \"range normalize\" between -1 and 1.\n",
    "for i in range(0, 4):\n",
    "    norm.make_col_numeric(iris_work, i)\n",
    "    norm.norm_col_range(iris_work, i, 0, 1)\n",
    "\n",
    "# Discover all of the classes for column #4, the iris species.\n",
    "classes = norm.build_class_map(iris_work, 4)\n",
    "inv_classes = {v: k for k, v in classes.items()}\n",
    "\n",
    "# Normalize iris species using one-of-n.\n",
    "# We could have used equilateral as well.  For an example of equilateral, see the example_nm_iris example.\n",
    "norm.norm_col_one_of_n(iris_work, 4, classes, 0, 1)\n",
    "\n",
    "# Prepare training data.  Separate into input and ideal.\n",
    "training = np.array(iris_work)\n",
    "training_input = training[:, 0:4]\n",
    "training_ideal = training[:, 4:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the score of the training process of the network\n",
    "def score_funct(x):\n",
    "    \"\"\"\n",
    "    The score function for Iris anneal.\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    global best_score\n",
    "    global input_data\n",
    "    global output_data\n",
    "    # Update the network's long term memory to the vector we need to score.\n",
    "    network.copy_memory(x)\n",
    "    # Loop over the training set and calculate the output for each.\n",
    "    actual_output = []\n",
    "    for input_data in training_input:\n",
    "        output_data = network.compute_regression(input_data)\n",
    "        actual_output.append(output_data)\n",
    "    # Calculate the error with MSE.\n",
    "    result = ErrorCalculation.mse(np.array(actual_output), training_ideal)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RBF network.  There are four inputs and two outputs.\n",
    "# There are also five RBF functions used internally.\n",
    "# You can experiment with different numbers of internal RBF functions.\n",
    "# However, the input and output must match the data set.\n",
    "inputs = 4\n",
    "rbfs = 4\n",
    "outputs = 3\n",
    "network = RbfNetwork(inputs, rbfs, outputs)\n",
    "network.reset()\n",
    "\n",
    "# Create a copy of the long-term memory.  This becomes the initial state.\n",
    "x0 = list(network.long_term_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the RBFN API please follow the next steps to train a RBF to clssify the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform the simmulated annealing.\n",
    "# Display the final validation.  We show all of the iris data as well as the predicted species.\n",
    "# Compute the output from the RBF network\n",
    "# Decode the three output neurons into a class number and print it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector quantization (VQ) is a form of competitive learning. Such an algorithm is able to discover structure in the input data. Generally speaking, vector quantization is a form of lossy data compression—lossy in the sense that some information contained in the input data is lost as a result of the compression.\n",
    "![title](img/vq_alg.png)\n",
    "An input data point belongs to a certain class if its position (in the 2D space) is closest to the class prototype, fulfilling the Voronoi partitioning (i.e. partitioning of a plane into regions based on distance to points in a specific subset of the plane.\n",
    "![title](img/vq.png)\n",
    "In a typical scenario, such behavior can be implemented with a neural network that consists of two layers—an input layer and a competitive layer with lateral inhibition. The input layer receives the available data. The competitive layer consists of neurons that compete with each other.\n",
    "![title](img/vq_net.png)\n",
    "The classic image processing example, Lena, an 8-bit grayscale bit-depth, 512 x 512 sized image, is used here to illustrate how `k`-means is used for vector quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn.utils.testing import SkipTest\n",
    "from sklearn.utils.fixes import sp_version\n",
    "\n",
    "try:\n",
    "    face = sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    # Newer versions of scipy have face in misc\n",
    "    from scipy import misc\n",
    "    face = misc.face(gray=True)\n",
    "\n",
    "n_clusters = 5\n",
    "np.random.seed(0)\n",
    "\n",
    "X = face.reshape((-1, 1))  # We need an (n_sample, n_feature) array\n",
    "k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\n",
    "k_means.fit(X)\n",
    "values = k_means.cluster_centers_.squeeze()\n",
    "labels = k_means.labels_\n",
    "\n",
    "# create an array from labels and values\n",
    "face_compressed = np.choose(labels, values)\n",
    "face_compressed.shape = face.shape\n",
    "\n",
    "vmin = face.min()\n",
    "vmax = face.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of the clutering and plot the original, quatized, and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# original face\n",
    "plt.figure(1, figsize=(3, 2.2))\n",
    "plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
    "\n",
    "# compressed face\n",
    "plt.figure(2, figsize=(3, 2.2))\n",
    "plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# equal bins face\n",
    "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
    "regular_labels = np.searchsorted(regular_values, face) - 1\n",
    "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
    "regular_face = np.choose(regular_labels.ravel(), regular_values, mode=\"clip\")\n",
    "regular_face.shape = face.shape\n",
    "plt.figure(3, figsize=(3, 2.2))\n",
    "plt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# histogram\n",
    "plt.figure(4, figsize=(3, 2.2))\n",
    "plt.clf()\n",
    "plt.axes([.01, .01, .98, .98])\n",
    "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
    "plt.yticks(())\n",
    "plt.xticks(regular_values)\n",
    "values = np.sort(values)\n",
    "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
    "\n",
    "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this problem you should group 2d input points (x,y) into clusters and determine the center of each cluster. The number of required clusters is provided as integer number on the first line. Following, the system provides an unknown number of 2d input data points (x, y), one per line. Continue reading until your program obtains no more data. You can safely assume to read less than 1000 points. After reading, you should run the Vector Quantization algorithm to find the center(s) of input data, and finally report the center position as x, y coordinate. Present one such center position per output line. The order of center points output does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 cluster VQ\n",
    "![title](img/vq_3clust.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the datasets for training and testing \n",
    "import numpy as np\n",
    "import csv \n",
    "with open('./data/vq_3clust_in.txt') as inputfile:\n",
    "    train_data = list(csv.reader(inputfile))\n",
    "with open('./data/vq_3clust_out.txt') as inputfile:\n",
    "    test_data = list(csv.reader(inputfile))\n",
    "    \n",
    "# add network code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 cluster VQ\n",
    "![title](img/vq_3clust.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the datasets for training and testing for the 6 cluster example\n",
    "import numpy as np\n",
    "import csv \n",
    "with open('./data/vq_6clust_in.txt') as inputfile:\n",
    "    train_data = list(csv.reader(inputfile))\n",
    "with open('./data/vq_6clust_out.txt') as inputfile:\n",
    "    test_data = list(csv.reader(inputfile))\n",
    "    \n",
    "# add network code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neurobiology, during neural growth, synapses are strengthened or weakened, in a process usually modelled as a competition for resources. In such a learning process, there is a competition between the neurons to fire. More precisely, neurons compete with each other (in accordance with a learning rule) for the “opportunity” to respond to features contained in the input data. \n",
    "![title](img/som.png)\n",
    "In its simplest form, such behaviour describes a “winner-takes-all” strategy. In such a strategy, the neuron with the greatest total input “wins” the competition and turns on; all the other neurons in the network then switch off. The aim of such learning mechanisms is to cluster the data.\n",
    "![title](img/som_tr.png)\n",
    "Kohonen’s self-organizing map (SOM) is one of the most popular unsupervised neural network models. Developed for an associative memory model, it is an unsupervised learning algorithm with a simple structure and computational form, and is motivated by the retina-cortex mapping. The SOM can provide topologically preserved mapping from input to output spaces, such that “nearby” sensory stimuli are represented in “nearby” regions.\n",
    "![title](img/som_alg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing a basic SOM\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "\n",
    "\n",
    "class SelfOrganizingMap:\n",
    "    \"\"\"\n",
    "    The weights of the output neurons base on the input from the input\n",
    "    neurons.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_count, output_count):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        :param input_count: Number of input neurons\n",
    "        :param output_count: Number of output neurons\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.input_count = input_count\n",
    "        self.output_count = output_count\n",
    "        self.weights = np.zeros([self.output_count, self.input_count])\n",
    "\n",
    "        self.distance = sp.spatial.distance.euclidean\n",
    "\n",
    "    def calculate_error(self, data):\n",
    "        bmu = BestMatchingUnit(self)\n",
    "\n",
    "        bmu.reset()\n",
    "\n",
    "        # Determine the BMU for each training element.\n",
    "        for input in data:\n",
    "            bmu.calculate_bmu(input)\n",
    "\n",
    "        # update the error\n",
    "        return bmu.worst_distance / 100.0\n",
    "\n",
    "    def classify(self, input):\n",
    "        if len(input) > self.input_count:\n",
    "            raise Exception(\"Can't classify SOM with input size of {} \"\n",
    "                            \"with input data of count {}\".format(self.input_count, len(input)))\n",
    "\n",
    "        min_dist = sys.maxfloat\n",
    "        result = -1\n",
    "\n",
    "        for i in range(self.output_count):\n",
    "            dist = self.distance.calculate(input, self.weights[i])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                result = i\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = (np.random.rand(self.weights.shape[0], self.weights.shape[1]) * 2.0) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Best Matching Unit\" or BMU is a very important concept in the training for a SOM. The BMU is the output neuron that has weight connections to the input neurons that most closely match the current input vector. This neuron (and its \"neighborhood\") are the neurons that will receive training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing the competition stage in SOM, finding the best matching unit.\n",
    "class BestMatchingUnit:\n",
    "    \"\"\"\n",
    "    This class also tracks the worst distance (of all BMU's). This gives some\n",
    "    indication of how well the network is trained, and thus becomes the \"error\"\n",
    "    of the entire network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, som):\n",
    "        \"\"\"\n",
    "        Construct a BestMatchingUnit class.  The training class must be provided.\n",
    "        :param som: The SOM to evaluate.\n",
    "        \"\"\"\n",
    "        # The owner of this class.\n",
    "        self.som = som\n",
    "\n",
    "        # What is the worst BMU distance so far, this becomes the error for the\n",
    "        # entire SOM.\n",
    "        self.worst_distance = 0\n",
    "\n",
    "    def calculate_bmu(self, input):\n",
    "        \"\"\"\n",
    "        Calculate the best matching unit (BMU). This is the output neuron that\n",
    "        has the lowest Euclidean distance to the input vector.\n",
    "        :param input: The input vector.\n",
    "        :return: The output neuron number that is the BMU.\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "\n",
    "        if len(input) > self.som.input_count:\n",
    "            raise Exception(\n",
    "                \"Can't train SOM with input size of {}  with input data of count {}.\".format(self.som.input_count,\n",
    "                                                                                             len(input)))\n",
    "\n",
    "        # Track the lowest distance so far.\n",
    "        lowest_distance = float(\"inf\")\n",
    "\n",
    "        for i in range(self.som.output_count):\n",
    "            distance = self.calculate_euclidean_distance(self.som.weights, input, i)\n",
    "\n",
    "            # Track the lowest distance, this is the BMU.\n",
    "            if distance < lowest_distance:\n",
    "                lowest_distance = distance\n",
    "                result = i\n",
    "\n",
    "        # Track the worst distance, this is the error for the entire network.\n",
    "        if lowest_distance > self.worst_distance:\n",
    "            self.worst_distance = lowest_distance\n",
    "\n",
    "        return result\n",
    "\n",
    "    def calculate_euclidean_distance(self, matrix, input, output_neuron):\n",
    "        \"\"\"\n",
    "        Calculate the Euclidean distance for the specified output neuron and the\n",
    "        input vector.  This is the square root of the squares of the differences\n",
    "        between the weight and input vectors.\n",
    "        :param matrix: The matrix to get the weights from.\n",
    "        :param input: The input vector.\n",
    "        :param outputNeuron: The neuron we are calculating the distance for.\n",
    "        :return: The Euclidean distance.\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "\n",
    "        # Loop over all input data.\n",
    "        diff = input - matrix[output_neuron]\n",
    "        return np.sqrt(sum(diff*diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we analyze competitive training, which would be used in a winner-take-all neural network, such as the self organizing map (SOM). This is an unsupervised training method, no ideal data is needed on the training set. If ideal data is provided, it will be ignored. Training is done by looping over all of the training elements and calculating a \"best matching unit\" (BMU). This BMU output neuron is then adjusted to better \"learn\" this pattern. Additionally, this training may be applied to other \"nearby\" output neurons. The degree to which nearby neurons are update is defined by the neighborhood function.\n",
    "\n",
    "A neighborhood function is required to determine the degree to which neighboring neurons (to the winning neuron) are updated by each training iteration. Because this is unsupervised training, calculating an error to measure progress by is difficult. The error is defined to be the \"worst\", or longest, Euclidean distance of any of the BMU's. This value should be minimized, as learning progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing the basic training algorithm for a SOM\n",
    "class BasicTrainSOM:\n",
    "    \"\"\"\n",
    "    Because only the BMU neuron and its close neighbors are updated, you can end\n",
    "    up with some output neurons that learn nothing. By default these neurons are\n",
    "    not forced to win patterns that are not represented well. This spreads out\n",
    "    the workload among all output neurons. This feature is not used by default,\n",
    "    but can be enabled by setting the \"forceWinner\" property.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, learning_rate, training, neighborhood):\n",
    "\n",
    "        # The neighborhood function to use to determine to what degree a neuron\n",
    "        # should be \"trained\".\n",
    "        self.neighborhood = neighborhood\n",
    "\n",
    "        # The learning rate. To what degree should changes be applied.\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # The network being trained.\n",
    "        self.network = network\n",
    "\n",
    "        # How many neurons in the input layer.\n",
    "        self.input_neuron_count = network.input_count\n",
    "\n",
    "        # How many neurons in the output layer.\n",
    "        self.output_neuron_count = network.output_count\n",
    "\n",
    "        # Utility class used to determine the BMU.\n",
    "        self.bmu_util = BestMatchingUnit(network)\n",
    "\n",
    "        # Correction matrix.\n",
    "        self.correction_matrix = np.zeros([network.output_count, network.input_count])\n",
    "\n",
    "        # True is a winner is to be forced, see class description, or forceWinners\n",
    "        # method. By default, this is true.\n",
    "        self.force_winner = False\n",
    "\n",
    "        # When used with autodecay, this is the starting learning rate.\n",
    "        self.start_rate = 0\n",
    "\n",
    "        # When used with autodecay, this is the ending learning rate.\n",
    "        self.end_rate = 0\n",
    "\n",
    "        # When used with autodecay, this is the starting radius.\n",
    "        self.start_radius = 0\n",
    "\n",
    "        # When used with autodecay, this is the ending radius.\n",
    "        self.end_radius = 0\n",
    "\n",
    "        # This is the current autodecay learning rate.\n",
    "        self.auto_decay_rate = 0\n",
    "\n",
    "        # This is the current autodecay radius.\n",
    "        self.auto_decay_radius = 0\n",
    "\n",
    "        # The current radius.\n",
    "        self.radius = 0\n",
    "\n",
    "        # Training data.\n",
    "        self.training = training\n",
    "\n",
    "    def _apply_correction(self):\n",
    "        \"\"\"\n",
    "        Loop over the synapses to be trained and apply any corrections that were\n",
    "        determined by this training iteration.\n",
    "        \"\"\"\n",
    "        np.copyto(self.network.weights, self.correction_matrix)\n",
    "\n",
    "    def auto_decay(self):\n",
    "        \"\"\"\n",
    "        Should be called each iteration if autodecay is desired.\n",
    "        \"\"\"\n",
    "        if self.radius > self.end_radius:\n",
    "            self.radius += self.auto_decay_radius\n",
    "\n",
    "        if self.learning_rate > self.end_rate:\n",
    "            self.learning_rate += self.auto_decay_rate\n",
    "\n",
    "        self.neighborhood.radius = self.radius\n",
    "\n",
    "    def copy_input_pattern(self, matrix, output_neuron, input):\n",
    "        \"\"\"\n",
    "        Copy the specified input pattern to the weight matrix. This causes an\n",
    "        output neuron to learn this pattern \"exactly\". This is useful when a\n",
    "        winner is to be forced.\n",
    "        :param matrix: The matrix that is the target of the copy.\n",
    "        :param output_neuron: The output neuron to set.\n",
    "        :param input: The input pattern to copy.\n",
    "        \"\"\"\n",
    "        matrix[output_neuron, :] = input\n",
    "\n",
    "    def decay(self, decay_rate, decay_radius):\n",
    "        \"\"\"\n",
    "        Decay the learning rate and radius by the specified amount.\n",
    "        :param decay_rate: The percent to decay the learning rate by.\n",
    "        :param decay_radius: The percent to decay the radius by.\n",
    "        \"\"\"\n",
    "        self.radius *= (1.0 - decay_radius)\n",
    "        self.learning_rate *= (1.0 - decay_rate)\n",
    "        self.neighborhood.radius = self.radius\n",
    "\n",
    "    def _determine_new_weight(self, weight, input, currentNeuron, bmu):\n",
    "        \"\"\"\n",
    "        Determine the weight adjustment for a single neuron during a training\n",
    "        iteration.\n",
    "        :param weight: The starting weight.\n",
    "        :param input: The input to this neuron.\n",
    "        :param currentNeuron: The neuron who's weight is being updated.\n",
    "        :param bmu: The neuron that \"won\", the best matching unit.\n",
    "        :return: The new weight value.\n",
    "        \"\"\"\n",
    "        return weight \\\n",
    "               + (self.neighborhood.fn(currentNeuron, bmu) \\\n",
    "                  * self.learning_rate * (input - weight))\n",
    "\n",
    "    def _force_winners(self, matrix, won, least_represented):\n",
    "        \"\"\"\n",
    "        Force any neurons that did not win to off-load patterns from overworked\n",
    "        neurons.\n",
    "        :param matrix: An array that specifies how many times each output neuron has \"won\".\n",
    "        :param won: The training pattern that is the least represented by this neural network.\n",
    "        :param least_represented: The synapse to modify.\n",
    "        :return: True if a winner was forced.\n",
    "        \"\"\"\n",
    "        max_activation = float(\"-inf\")\n",
    "        max_activation_neuron = -1\n",
    "\n",
    "        output = self.compute(self.network, self.least_represented)\n",
    "\n",
    "        # Loop over all of the output neurons. Consider any neurons that were\n",
    "        # not the BMU (winner) for any pattern. Track which of these\n",
    "        # non-winning neurons had the highest activation.\n",
    "        for output_neuron in range(len(won)):\n",
    "            # Only consider neurons that did not \"win\".\n",
    "            if won[output_neuron] == 0:\n",
    "                if (max_activation_neuron == -1) \\\n",
    "                        or (output[output_neuron] > max_activation):\n",
    "                    max_activation = output[output_neuron]\n",
    "                    max_activation_neuron = output_neuron\n",
    "\n",
    "        # If a neurons was found that did not activate for any patterns, then\n",
    "        # force it to \"win\" the least represented pattern.\n",
    "        if max_activation_neuron != -1:\n",
    "            self.copy_input_pattern(matrix, max_activation_neuron, least_represented)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def iteration(self):\n",
    "        \"\"\"\n",
    "        Perform one training iteration.\n",
    "        \"\"\"\n",
    "        # Reset the BMU and begin this iteration.\n",
    "        self.bmu_util.reset()\n",
    "        won = [0] * self.output_neuron_count\n",
    "        least_represented_activation = float(\"inf\")\n",
    "        least_represented = None\n",
    "\n",
    "        # Reset the correction matrix for this synapse and iteration.\n",
    "        self.correctionMatrix.clear()\n",
    "\n",
    "        # Determine the BMU for each training element.\n",
    "        for input in self.training:\n",
    "            bmu = self.bmu_util.calculate_bmu(input)\n",
    "            won[bmu] += 1\n",
    "\n",
    "            # If we are to force a winner each time, then track how many\n",
    "            # times each output neuron becomes the BMU (winner).\n",
    "            if self.force_winner:\n",
    "                # Get the \"output\" from the network for this pattern. This\n",
    "                # gets the activation level of the BMU.\n",
    "                output = self.compute(self.network, input)\n",
    "\n",
    "                # Track which training entry produces the least BMU. This\n",
    "                # pattern is the least represented by the network.\n",
    "                if output[bmu] < least_represented_activation:\n",
    "                    least_represented_activation = output[bmu]\n",
    "                    least_represented = input.getInput()\n",
    "\n",
    "            self.train(bmu, self.network.getWeights(), input.getInput())\n",
    "\n",
    "            if self.force_winner:\n",
    "                # force any non-winning neurons to share the burden somewhat\n",
    "                if not self.force_winners(self.network.weights, won, least_represented):\n",
    "                    self.apply_correction()\n",
    "            else:\n",
    "                self.apply_correction()\n",
    "\n",
    "    def set_auto_decay(self, planned_iterations, start_rate, end_rate, start_radius, end_radius):\n",
    "        \"\"\"\n",
    "        Setup autodecay. This will decrease the radius and learning rate from the\n",
    "        start values to the end values.\n",
    "        :param planned_iterations: The number of iterations that are planned. This allows the\n",
    "        decay rate to be determined.\n",
    "        :param start_rate: The starting learning rate.\n",
    "        :param end_rate: The ending learning rate.\n",
    "        :param start_radius: The starting radius.\n",
    "        :param end_radius: The ending radius.\n",
    "        \"\"\"\n",
    "        self.start_rate = start_rate\n",
    "        self.end_rate = end_rate\n",
    "        self.start_radius = start_radius\n",
    "        self.end_radius = end_radius\n",
    "        self.auto_decay_radius = (end_radius - start_radius) / planned_iterations\n",
    "        self.auto_decay_rate = (end_rate - start_rate) / planned_iterations\n",
    "        self.set_params(self.start_rate, self.start_radius)\n",
    "\n",
    "    def set_params(self, rate, radius):\n",
    "        \"\"\"\n",
    "        Set the learning rate and radius.\n",
    "        :param rate: The new learning rate.\n",
    "        :param radius:\n",
    "        :return: The new radius.\n",
    "        \"\"\"\n",
    "        self.radius = radius\n",
    "        self.learning_rate = rate\n",
    "        self.neighborhood.radius = radius\n",
    "\n",
    "    def get_status(self):\n",
    "        \"\"\"\n",
    "        :return: A string display of the status.\n",
    "        \"\"\"\n",
    "        result = \"Rate=\"\n",
    "        result += str(self.learning_rate)\n",
    "        result += \", Radius=\"\n",
    "        result += str(self.radius)\n",
    "        return result\n",
    "\n",
    "    def _train(self, bmu, matrix, input):\n",
    "        \"\"\"\n",
    "        Train for the specified synapse and BMU.\n",
    "        :param bmu: The best matching unit for this input.\n",
    "        :param matrix: The synapse to train.\n",
    "        :param input: The input to train for.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # adjust the weight for the BMU and its neighborhood\n",
    "        for output_neuron in range(self.output_neuron_count):\n",
    "            self._train_pattern(matrix, input, output_neuron, bmu)\n",
    "\n",
    "    def _train_pattern(self, matrix, input, current, bmu):\n",
    "        \"\"\"\n",
    "        Train for the specified pattern.\n",
    "        :param matrix: The synapse to train.\n",
    "        :param input: The input pattern to train for.\n",
    "        :param current: The current output neuron being trained.\n",
    "        :param bmu: The best matching unit, or winning output neuron.\n",
    "        \"\"\"\n",
    "\n",
    "        for input_neuron in range(self.input_neuron_count):\n",
    "            current_weight = matrix[current][input_neuron]\n",
    "            input_value = input[input_neuron]\n",
    "\n",
    "            new_weight = self._determine_new_weight(current_weight,\n",
    "                    input_value, current, bmu)\n",
    "\n",
    "            self.correction_matrix[current][input_neuron] = new_weight\n",
    "\n",
    "    def train_single_pattern(self, pattern):\n",
    "        \"\"\"\n",
    "        Train the specified pattern. Find a winning neuron and adjust all neurons\n",
    "        according to the neighborhood function.\n",
    "        :param pattern: The pattern to train.\n",
    "        \"\"\"\n",
    "        bmu = self.bmu_util.calculate_bmu(pattern)\n",
    "        self._train(bmu, self.network.weights, pattern)\n",
    "        self._apply_correction()\n",
    "\n",
    "    def compute(self, som, input):\n",
    "        \"\"\"\n",
    "        Calculate the output of the SOM, for each output neuron.  Typically,\n",
    "        you will use the classify method instead of calling this method.\n",
    "        :param som: The input pattern.\n",
    "        :param input: The output activation of each output neuron.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        result = np.zeros(som.output_count)\n",
    "\n",
    "        for i in range(som.output_count):\n",
    "            optr = som.weights[i]\n",
    "\n",
    "            matrix_a = np.zeros([input.length,1])\n",
    "            for j in range(len(input)):\n",
    "                matrix_a[0][j] = input[j]\n",
    "\n",
    "            matrix_b = np.zeros(1,input.length)\n",
    "            for j in range(len(optr)):\n",
    "                matrix_b[0][j] = optr[j]\n",
    "\n",
    "            result[i] = np.dot(matrix_a, matrix_b)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common example used to help teach the principals behind SOMs is the mapping of colours from their three dimensional components - red, green and blue, into two dimensions.The colours are presented to the network as 3D vectors - one dimension for each of the colour components (RGB encoding) - and the network learns to represent them in the 2D space we can see. Notice that in addition to clustering the colours into distinct regions, regions of similar properties are usually found adjacent to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from Tkinter import *\n",
    "import numpy as np\n",
    "from neighborhood import *\n",
    "\n",
    "TILES_WIDTH = 50\n",
    "TILES_HEIGHT = 50\n",
    "TILE_SCREEN_SIZE = 10\n",
    "\n",
    "class DisplayColors:\n",
    "    def __init__(self,root,samples):\n",
    "        # Build the grid display\n",
    "        canvas_width = TILES_WIDTH * TILE_SCREEN_SIZE\n",
    "        canvas_height = TILES_HEIGHT * TILE_SCREEN_SIZE\n",
    "\n",
    "        self.samples = samples\n",
    "        self.root = root\n",
    "        self.c = Canvas(self.root,width=canvas_width, height=canvas_height)\n",
    "        self.c.pack()\n",
    "\n",
    "        self.grid_rects = [[None for j in range(TILES_WIDTH)]\n",
    "            for i in range(TILES_HEIGHT)]\n",
    "\n",
    "        for row in range(TILES_HEIGHT):\n",
    "            for col in range(TILES_WIDTH):\n",
    "                x = col * TILE_SCREEN_SIZE\n",
    "                y = row * TILE_SCREEN_SIZE\n",
    "                r = self.c.create_rectangle(x, y, x+TILE_SCREEN_SIZE,y+TILE_SCREEN_SIZE, fill=\"white\")\n",
    "                self.grid_rects[row][col] = r\n",
    "\n",
    "        self.som = SelfOrganizingMap(3,TILES_WIDTH * TILES_HEIGHT)\n",
    "        self.som.reset()\n",
    "\n",
    "        self.gaussian = NeighborhoodRBF(NeighborhoodRBF.TYPE_GAUSSIAN,[TILES_WIDTH,TILES_HEIGHT])\n",
    "        self.train = BasicTrainSOM(self.som, 0.01, None, self.gaussian)\n",
    "        self.train.force_winner = False\n",
    "        self.train.set_auto_decay(1000, 0.8, 0.003, 30, 5)\n",
    "        self.iteration = 1\n",
    "\n",
    "    def RGBToHTMLColor(self, rgb_tuple):\n",
    "        hexcolor = '#%02x%02x%02x' % rgb_tuple\n",
    "        return hexcolor\n",
    "\n",
    "    def convert_color(self, d):\n",
    "        result = 128*d\n",
    "        result+= 128\n",
    "        result = min(result, 255)\n",
    "        result = max(result, 0)\n",
    "        return result\n",
    "\n",
    "    def update(self, som):\n",
    "        for row in range(TILES_HEIGHT):\n",
    "            for col in range(TILES_WIDTH):\n",
    "                index = (row*TILES_WIDTH)+col\n",
    "                color = (\n",
    "                    self.convert_color(som.weights[index][0]),\n",
    "                    self.convert_color(som.weights[index][1]),\n",
    "                    self.convert_color(som.weights[index][2]))\n",
    "                r = self.grid_rects[row][col]\n",
    "                self.c.itemconfig(r, fill=self.RGBToHTMLColor(color))\n",
    "                self.c.itemconfig(r, outline=self.RGBToHTMLColor(color))\n",
    "\n",
    "    def update_clock(self):\n",
    "        idx = np.random.randint(len(samples))\n",
    "        c = self.samples[idx]\n",
    "\n",
    "        self.train.train_single_pattern(c)\n",
    "        self.train.auto_decay()\n",
    "        self.update(self.som)\n",
    "        print(\"Iteration {}, {}\".format(self.iteration,self.train.get_status()))\n",
    "        self.iteration+=1\n",
    "        if self.iteration<=1000:\n",
    "            self.root.after(1, self.update_clock)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "samples = np.zeros([15,3])\n",
    "for i in range(15):\n",
    "    samples[i][0] = np.random.uniform(-1,1)\n",
    "    samples[i][1] = np.random.uniform(-1,1)\n",
    "    samples[i][2] = np.random.uniform(-1,1)\n",
    "\n",
    "root = Tk()\n",
    "display = DisplayColors(root, samples)\n",
    "display.update_clock()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this assignment a solution path for the Traveling Salesman Problem (finding a short path to travel once to each city and return home), for an unknown number of cities as input (you can safely assume <= 1000 cities). Each city consists of an ID (an integer number), and X and Y position of that city (two integer numbers). The provided input format for each line to read in is CITY-ID,X,Y\\n.\n",
    "\n",
    "Your program shall implement a Self-Organizing Map to accomplish this task. When your SOM finished learning, print the path as one city-id per line, followed by '\\n'. Example for three cities with IDs 1,2,3 which are visited in the order 3,1,2:\n",
    "\n",
    "  3\\n\n",
    "  1\\n\n",
    "  2\\n\n",
    "\n",
    "Remember that the number of cities in the output corresponds exactly to the number of cities in the input. It does not matter which of the cities is the first on your path.\n",
    "You can safely assume that your program does not need to find the shortest possible path (remember, this problem is NP hard!), but your result needs to be within 15% of the shortest path we found (which again might not be optimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travelling salesmap across Europe :)\n",
    "![title](img/som_ts_eu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the datasets for training and testing for TS in Europe\n",
    "import numpy as np\n",
    "import csv \n",
    "with open('./data/som_ts_in.txt') as inputfile:\n",
    "    train_data = list(csv.reader(inputfile))\n",
    "with open('./data/som_ts_out.txt') as inputfile:\n",
    "    test_data = list(csv.reader(inputfile))\n",
    "    \n",
    "# add network code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for a more complex example, consider a more restricted dataset.\n",
    "![title](img/som_ts_random.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the datasets for training and testing for TS \n",
    "import numpy as np\n",
    "import csv \n",
    "with open('./data/som_ts_in_aux.txt') as inputfile:\n",
    "    train_data = list(csv.reader(inputfile))\n",
    "with open('./data/som_ts_out_aux.txt') as inputfile:\n",
    "    test_data = list(csv.reader(inputfile))\n",
    "    \n",
    "# add network code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Hebb hypothesized in 1949 how neurons are connected with each other in the brain: “When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”, and postulated a new learning mechanism, Hebbian learning. \n",
    "\n",
    "In other words neural networks stores and retrieves associations, which are learned as synaptic connection. In Hebbian learning, both presynaptic and postsynaptic neurons are involved. Human memory thus works in an associative or content-addressable way.\n",
    "\n",
    "The model is a recurrent neural network with fully interconnected neurons. The number of feedback loops is equal to the number of neurons. Basically, the output of each neuron is fed back, via a unit-time delay element, to each of the other neurons in the network. \n",
    "![title](img/hopfield.png)\n",
    "Such a structure allows the network to recognise any of the learned patterns by exposure to only partial or even some corrupted information about that pattern, i.e., it eventually settles down and returns the closest pattern or the best guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class implementing a Hopfield Network\n",
    "import numpy as np\n",
    "from energetic import EnergeticNetwork\n",
    "\n",
    "class HopfieldNetwork(EnergeticNetwork):\n",
    "    def __init__(self, neuron_count):\n",
    "        EnergeticNetwork.__init__(self, neuron_count)\n",
    "        self.input_count = neuron_count\n",
    "        self.output_count = neuron_count\n",
    "        self.activation_function = lambda d: 1 if (d > 0) else 0\n",
    "\n",
    "    def compute(self, input):\n",
    "        \"\"\"\n",
    "        Note: for Hopfield networks, you will usually want to call the \"run\"\n",
    "        method to compute the output.\n",
    "        This method can be used to copy the input data to the current state. A\n",
    "        single iteration is then run, and the new current state is returned.\n",
    "        :param input: The input pattern.\n",
    "        :return: The new current state.\n",
    "        \"\"\"\n",
    "        result = self.current_state[:]\n",
    "        self.run()\n",
    "\n",
    "        for i in range(self.current_state):\n",
    "            result[i] = self.activation_function(self.current_state[i])\n",
    "\n",
    "        self.current_state[:] = result\n",
    "        return result\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Perform one Hopfield iteration.\n",
    "        \"\"\"\n",
    "        for to_neuron in range(self.neuron_count):\n",
    "            sum = 0\n",
    "            for from_neuron in range(self.neuron_count):\n",
    "                sum += self.current_state[from_neuron] \\\n",
    "                    * self.get_weight(from_neuron, to_neuron)\n",
    "                self.current_state[to_neuron] = self.activation_function(sum)\n",
    "\n",
    "    def run_until_stable(self, max_cycle):\n",
    "        \"\"\"\n",
    "        Run the network until it becomes stable and does not change from more runs.\n",
    "        :param max_cycle: The maximum number of cycles to run before giving up.\n",
    "        :return: The number of cycles that were run.\n",
    "        \"\"\"\n",
    "\n",
    "        done = False\n",
    "        last_state_str = str(self.current_state)\n",
    "        current_state_str = last_state_str\n",
    "\n",
    "        cycle = 0\n",
    "        while not done:\n",
    "            self.run()\n",
    "            cycle += 1\n",
    "\n",
    "            last_state_str = str(self.current_state)\n",
    "\n",
    "            if last_state_str == current_state_str:\n",
    "                if cycle > max_cycle:\n",
    "                    done = True\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "            current_state_str = last_state_str\n",
    "\n",
    "        return cycle\n",
    "\n",
    "    def energy(self):\n",
    "        t = 0\n",
    "\n",
    "        # Calculate first term\n",
    "        a = 0\n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.output_count):\n",
    "                a += self.get_weight(i, j) * self.current_state[i] * self.current_state[j]\n",
    "        a *= -0.5\n",
    "\n",
    "        # Calculate second term\n",
    "        b = 0\n",
    "        for i in range(self.input_count):\n",
    "            b += self.current_state[i] * t\n",
    "        return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we implement the Hopefield Network training algorithm\n",
    "![title](img/hopfield_alg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainHopfieldHebbian:\n",
    "    def __init__(self, network):\n",
    "        self.network = network;\n",
    "        self.sum_matrix = np.zeros([network.input_count, network.input_count])\n",
    "        self.pattern_count = 1\n",
    "\n",
    "    def add_pattern(self, pattern):\n",
    "        for i in range(self.network.input_count):\n",
    "            for j in range(self.network.input_count):\n",
    "                if i == j:\n",
    "                    self.sum_matrix[i][j] = 0\n",
    "                else:\n",
    "                    self.sum_matrix[i][j] += pattern[i] * pattern[j]\n",
    "\n",
    "        self.pattern_count += 1\n",
    "\n",
    "    def learn(self):\n",
    "        if self.pattern_count == 0:\n",
    "            raise Exception(\"Please add a pattern before learning.  Nothing to learn.\")\n",
    "\n",
    "        for i in range(self.network.input_count):\n",
    "            for j in range(self.network.input_count):\n",
    "                self.network.set_weight(i, j, self.sum_matrix[i][j]/self.pattern_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sample problem we will implement a Hopfield network to correct distorted patterns (here: 2D images). The algorithm reads a collection of binary images (5 patterns), each image being 10x10 \"pixels\" in size. A pixel may either be a space ' ' or a circle 'o'. \n",
    "\n",
    "We will train a Hopfield network (size 10x10 neurons) with these images as attractors. After training, the algorithm will read another small number of images with \"distortions\"; i.e. with incorrect pixel patterns compared to the previously trained images. For each such \"distorted\" image the algorithm shall output the closest training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The neural network will learn these patterns.\n",
    "PATTERN =  [[\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\"],\n",
    "\n",
    "          [ \"OO  OO  OO\",\n",
    "            \"OO  OO  OO\",\n",
    "            \"  OO  OO  \",\n",
    "            \"  OO  OO  \",\n",
    "            \"OO  OO  OO\",\n",
    "            \"OO  OO  OO\",\n",
    "            \"  OO  OO  \",\n",
    "            \"  OO  OO  \",\n",
    "            \"OO  OO  OO\",\n",
    "            \"OO  OO  OO\"  ],\n",
    "\n",
    "          [ \"OOOOO     \",\n",
    "            \"OOOOO     \",\n",
    "            \"OOOOO     \",\n",
    "            \"OOOOO     \",\n",
    "            \"OOOOO     \",\n",
    "            \"     OOOOO\",\n",
    "            \"     OOOOO\",\n",
    "            \"     OOOOO\",\n",
    "            \"     OOOOO\",\n",
    "            \"     OOOOO\"  ],\n",
    "\n",
    "          [ \"O  O  O  O\",\n",
    "            \" O  O  O  \",\n",
    "            \"  O  O  O \",\n",
    "            \"O  O  O  O\",\n",
    "            \" O  O  O  \",\n",
    "            \"  O  O  O \",\n",
    "            \"O  O  O  O\",\n",
    "            \" O  O  O  \",\n",
    "            \"  O  O  O \",\n",
    "            \"O  O  O  O\"  ],\n",
    "\n",
    "          [ \"OOOOOOOOOO\",\n",
    "            \"O        O\",\n",
    "            \"O OOOOOO O\",\n",
    "            \"O O    O O\",\n",
    "            \"O O OO O O\",\n",
    "            \"O O OO O O\",\n",
    "            \"O O    O O\",\n",
    "            \"O OOOOOO O\",\n",
    "            \"O        O\",\n",
    "            \"OOOOOOOOOO\"  ]]\n",
    "\n",
    "# The neural network will be tested on these patterns, to see which of the last set they are the closest to.\n",
    "PATTERN2 = [[\n",
    "            \"          \",\n",
    "            \"          \",\n",
    "            \"          \",\n",
    "            \"          \",\n",
    "            \"          \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\",\n",
    "            \"O O O O O \",\n",
    "            \" O O O O O\"],\n",
    "\n",
    "           [\"OOO O    O\",\n",
    "            \" O  OOO OO\",\n",
    "            \"  O O OO O\",\n",
    "            \" OOO   O  \",\n",
    "            \"OO  O  OOO\",\n",
    "            \" O OOO   O\",\n",
    "            \"O OO  O  O\",\n",
    "            \"   O OOO  \",\n",
    "            \"OO OOO  O \",\n",
    "            \" O  O  OOO\"],\n",
    "\n",
    "           [\"OOOOO     \",\n",
    "            \"O   O OOO \",\n",
    "            \"O   O OOO \",\n",
    "            \"O   O OOO \",\n",
    "            \"OOOOO     \",\n",
    "            \"     OOOOO\",\n",
    "            \" OOO O   O\",\n",
    "            \" OOO O   O\",\n",
    "            \" OOO O   O\",\n",
    "            \"     OOOOO\"],\n",
    "\n",
    "           [\"O  OOOO  O\",\n",
    "            \"OO  OOOO  \",\n",
    "            \"OOO  OOOO \",\n",
    "            \"OOOO  OOOO\",\n",
    "            \" OOOO  OOO\",\n",
    "            \"  OOOO  OO\",\n",
    "            \"O  OOOO  O\",\n",
    "            \"OO  OOOO  \",\n",
    "            \"OOO  OOOO \",\n",
    "            \"OOOO  OOOO\"],\n",
    "\n",
    "           [\"OOOOOOOOOO\",\n",
    "            \"O        O\",\n",
    "            \"O        O\",\n",
    "            \"O        O\",\n",
    "            \"O   OO   O\",\n",
    "            \"O   OO   O\",\n",
    "            \"O        O\",\n",
    "            \"O        O\",\n",
    "            \"O        O\",\n",
    "            \"OOOOOOOOOO\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the image representation into a bipolar {-1/1} representation and display according to the original patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the network \n",
    "HEIGHT = 10\n",
    "WIDTH = 10\n",
    "\n",
    "def convert_pattern(data, index):\n",
    "    result_index = 0\n",
    "    result = np.zeros([WIDTH*HEIGHT])\n",
    "    for row in range(HEIGHT):\n",
    "        for col in range(WIDTH):\n",
    "            ch = data[index][row][col]\n",
    "            result[result_index] = 1 if ch != ' ' else -1\n",
    "            result_index += 1\n",
    "    return result\n",
    "\n",
    "def display(pattern1, pattern2):\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "\n",
    "    for row in range(HEIGHT):\n",
    "        line = \"\"\n",
    "        for col in range(WIDTH):\n",
    "            if pattern1[index1]>0:\n",
    "                line += \"O\"\n",
    "            else:\n",
    "                line += \" \"\n",
    "            index1 += 1\n",
    "\n",
    "        line += \"  ->   \"\n",
    "\n",
    "        for col in range(WIDTH):\n",
    "            if pattern2[index2] >0 :\n",
    "                line += \"O\"\n",
    "            else:\n",
    "                line += \" \"\n",
    "            index2 += 1\n",
    "\n",
    "        print(line)\n",
    "        \n",
    "        \n",
    "def display_data(pattern1):\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "\n",
    "    for row in range(HEIGHT):\n",
    "        line = \"\"\n",
    "        for col in range(WIDTH):\n",
    "            if pattern1[index1]>0:\n",
    "                line += \"O\"\n",
    "            else:\n",
    "                line += \" \"\n",
    "            index1 += 1\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the network for the provided patterns, using a number of N  steps of convergence\n",
    "N = 10\n",
    "\n",
    "def evaluate(hopfield, pattern):\n",
    "    for i in range(len(pattern)):\n",
    "        print 'Convergence for pattern %d \\n' % i\n",
    "        pattern1 = convert_pattern(pattern, i)\n",
    "        print 'input\\n'\n",
    "        display_data(pattern1)\n",
    "        hopfield.current_state = pattern1\n",
    "        cycles = hopfield.run_until_stable(N)\n",
    "        pattern2 = hopfield.current_state\n",
    "        print 'attractor\\n'\n",
    "        display_data(pattern2)\n",
    "        print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate distorted patterns\n",
      "\n",
      "Convergence for pattern 0 \n",
      "\n",
      "input\n",
      "\n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "attractor\n",
      "\n",
      "O O O O O \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "O O O O O \n",
      " O O O O O\n",
      "----------------------\n",
      "Convergence for pattern 1 \n",
      "\n",
      "input\n",
      "\n",
      "OOO O    O\n",
      " O  OOO OO\n",
      "  O O OO O\n",
      " OOO   O  \n",
      "OO  O  OOO\n",
      " O OOO   O\n",
      "O OO  O  O\n",
      "   O OOO  \n",
      "OO OOO  O \n",
      " O  O  OOO\n",
      "attractor\n",
      "\n",
      "OO  OO  OO\n",
      "OO  OO  OO\n",
      "  OO  OO  \n",
      "  OO  OO  \n",
      "OO  OO  OO\n",
      "OO  OO  OO\n",
      "  OO  OO  \n",
      "  OO  OO  \n",
      "OO  OO  OO\n",
      "OO  OO  OO\n",
      "----------------------\n",
      "Convergence for pattern 2 \n",
      "\n",
      "input\n",
      "\n",
      "OOOOO     \n",
      "O   O OOO \n",
      "O   O OOO \n",
      "O   O OOO \n",
      "OOOOO     \n",
      "     OOOOO\n",
      " OOO O   O\n",
      " OOO O   O\n",
      " OOO O   O\n",
      "     OOOOO\n",
      "attractor\n",
      "\n",
      "OOOOO     \n",
      "OOOOO     \n",
      "OOOOO     \n",
      "OOOOO     \n",
      "OOOOO     \n",
      "     OOOOO\n",
      "     OOOOO\n",
      "     OOOOO\n",
      "     OOOOO\n",
      "     OOOOO\n",
      "----------------------\n",
      "Convergence for pattern 3 \n",
      "\n",
      "input\n",
      "\n",
      "O  OOOO  O\n",
      "OO  OOOO  \n",
      "OOO  OOOO \n",
      "OOOO  OOOO\n",
      " OOOO  OOO\n",
      "  OOOO  OO\n",
      "O  OOOO  O\n",
      "OO  OOOO  \n",
      "OOO  OOOO \n",
      "OOOO  OOOO\n",
      "attractor\n",
      "\n",
      "O  O  O  O\n",
      " O  O  O  \n",
      "  O  O  O \n",
      "O  O  O  O\n",
      " O  O  O  \n",
      "  O  O  O \n",
      "O  O  O  O\n",
      " O  O  O  \n",
      "  O  O  O \n",
      "O  O  O  O\n",
      "----------------------\n",
      "Convergence for pattern 4 \n",
      "\n",
      "input\n",
      "\n",
      "OOOOOOOOOO\n",
      "O        O\n",
      "O        O\n",
      "O        O\n",
      "O   OO   O\n",
      "O   OO   O\n",
      "O        O\n",
      "O        O\n",
      "O        O\n",
      "OOOOOOOOOO\n",
      "attractor\n",
      "\n",
      "OOOOOOOOOO\n",
      "O        O\n",
      "O OOOOOO O\n",
      "O O    O O\n",
      "O O OO O O\n",
      "O O OO O O\n",
      "O O    O O\n",
      "O OOOOOO O\n",
      "O        O\n",
      "OOOOOOOOOO\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Create the network and train it on the first set of patterns and evaluate for both datasets (i.e. one correct and one distorted)\n",
    "hopfield = HopfieldNetwork(WIDTH*HEIGHT)\n",
    "train = TrainHopfieldHebbian(hopfield)\n",
    "\n",
    "for i in range(len(PATTERN)):\n",
    "    train.add_pattern(convert_pattern(PATTERN, i))\n",
    "train.learn()\n",
    "\n",
    "print(\"Evaluate distorted patterns\\n\")\n",
    "evaluate(hopfield, PATTERN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the application of the Hopfield network as a content-addressable memory, we know a priori the fixed points (attractors) of the network in that they correspond to the patterns to be stored. However, the synaptic weights of the network that produce the desired fixed points are unknown, and the problem is how to determine them. The primary function of a content-addressable memory is to retrieve a pattern (item) stored in memory in response to the presentation of an incomplete or noisy version of that pattern.\n",
    "![title](img/hopfield_energy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this assignment you should develop a Hopfield Network capable of learning a phonebook. More precisely, a simple autoassociative memory to recover names and phone numbers and/or match them.\n",
    "\n",
    "Assuming that this is the phonebook extract the network needs to learn:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TINA  -> 6843726\n",
    "\n",
    "ANTJE -> 8034673\n",
    "\n",
    "LISA  -> 7260915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code a Hopfield Network for phonebook learning and restoring using its Content-Addressable-Memory behavior. Simulate network for distorted numbers.\n",
    "\n",
    "The data is represented as: \n",
    "\n",
    " Input | Output \n",
    " \n",
    " Name -> Number \n",
    "\n",
    "TINA  -> ? 86'GV  |  TINA  -> 6843726\n",
    "\n",
    "ANTJE -> ?Z!ES-=  |  ANTJE -> 8034673\n",
    "\n",
    "LISA  ->  JK#XMG  |  LISA  -> 7260915\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate network for distorted name.\n",
    "\n",
    "The data is represented as: \n",
    "\n",
    " Input | Output \n",
    " \n",
    " Number -> Name\n",
    "\n",
    "6843726 -> ; 01,  |  6843726 -> TINA \n",
    "\n",
    "8034673 -> &;A$T  |  8034673 -> ANTJE\n",
    "\n",
    "7260915 -> N\";SE  |  7260915 -> LISA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate network for distorted names and numbers.\n",
    "\n",
    "The data is represented as: \n",
    "\n",
    " Input | Output \n",
    " \n",
    " Name -> Number \n",
    "\n",
    "TINE  -> 1F&KV]:  |  TINA  -> 6843726\n",
    "\n",
    "ANNJE -> %VZAQ$>  |  ANTJE -> 8034673\n",
    "\n",
    "RITA  -> [)@)EK&  |  DIVA  -> 6060737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
